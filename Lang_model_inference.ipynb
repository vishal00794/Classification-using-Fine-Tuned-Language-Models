{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 58779,
     "status": "ok",
     "timestamp": 1767160701756,
     "user": {
      "displayName": "Vishal Kumar",
      "userId": "08498631610373926685"
     },
     "user_tz": -330
    },
    "id": "q0V2P1jU-M5e",
    "outputId": "d9356935-7c96-4bd1-bd2a-e8e4ed423387"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive') #, force_remount=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 17249,
     "status": "ok",
     "timestamp": 1767160719002,
     "user": {
      "displayName": "Vishal Kumar",
      "userId": "08498631610373926685"
     },
     "user_tz": -330
    },
    "id": "Nz2Ksw8j71H3",
    "outputId": "f3db2868-ef0b-496f-ec71-c11047780a0c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
      "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
      "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "  Building wheel for unsloth (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
     ]
    }
   ],
   "source": [
    "!pip install -q unsloth\n",
    "# Also get the latest nightly Unsloth!\n",
    "!pip install -q --force-reinstall --no-cache-dir --no-deps git+https://github.com/unslothai/unsloth.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 10064,
     "status": "ok",
     "timestamp": 1767160729067,
     "user": {
      "displayName": "Vishal Kumar",
      "userId": "08498631610373926685"
     },
     "user_tz": -330
    },
    "id": "6j4bH8R575iz",
    "outputId": "66a6d411-b557-4d1b-e569-e35b49e8e34d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2025.12.9: Fast Llama patching. Transformers: 4.57.3.\n",
      "   \\\\   /|    Tesla T4. Num GPUs = 1. Max memory: 14.741 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.9.1+cu128. CUDA: 7.5. CUDA Toolkit: 12.8. Triton: 3.5.1\n",
      "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.33.post2. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "max_seq_length = 2048 # Choose any! Unsloth also supports RoPE (Rotary Positinal Embedding) scaling internally.\n",
    "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    "load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"unsloth/Llama-3.2-1B-Instruct\", # or choose \"unsloth/Llama-3.2-1B-Instruct\"\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit, # Will load the 4Bit Quantized Model\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 264,
     "status": "ok",
     "timestamp": 1767160777682,
     "user": {
      "displayName": "Vishal Kumar",
      "userId": "08498631610373926685"
     },
     "user_tz": -330
    },
    "id": "Svt1Y56y9tzw",
    "outputId": "6ad03114-a968-4282-d809-961932ee7ebc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/drive/MyDrive\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "%cd /content/drive/MyDrive\n",
    "# !ls\n",
    "import pandas as pd\n",
    "# train_spam_data = pd.read_csv('train_spam_data.csv')\n",
    "test_spam_data = pd.read_csv('test_spam_data.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1767160862038,
     "user": {
      "displayName": "Vishal Kumar",
      "userId": "08498631610373926685"
     },
     "user_tz": -330
    },
    "id": "HWWTIETi_l8K"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "import torch, csv, re\n",
    "CSV_PATH = \"test_spam_data.csv\"   # change this\n",
    "TEXT_COLUMN = \"text\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 30784,
     "status": "ok",
     "timestamp": 1767162710672,
     "user": {
      "displayName": "Vishal Kumar",
      "userId": "08498631610373926685"
     },
     "user_tz": -330
    },
    "id": "K55Da1gcGOPq",
    "outputId": "f53bd412-364e-41e2-d849-8308a7ea99a0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: SPAM\n",
      "Prediction: HAM\n",
      "Prediction: HAM\n",
      "Prediction: HAM\n",
      "Prediction: HAM\n",
      "Prediction: HAM\n",
      "Prediction: HAM\n",
      "Prediction: SPAM\n",
      "Prediction: HAM\n",
      "Prediction: HAM\n",
      "Prediction: HAM\n",
      "Prediction: HAM\n",
      "Prediction: HAM\n",
      "Prediction: SPAM\n",
      "Prediction: SPAM\n",
      "Prediction: SPAM\n",
      "Prediction: HAM\n",
      "Prediction: HAM\n",
      "Prediction: HAM\n",
      "Prediction: HAM\n",
      "Prediction: HAM\n",
      "Prediction: HAM\n",
      "Prediction: HAM\n",
      "Prediction: HAM\n",
      "Prediction: HAM\n",
      "Prediction: HAM\n",
      "Prediction: SPAM\n",
      "Prediction: HAM\n",
      "Prediction: SPAM\n",
      "Prediction: SPAM\n",
      "Prediction: HAM\n",
      "Prediction: HAM\n",
      "Prediction: HAM\n",
      "Prediction: SPAM\n",
      "Prediction: HAM\n",
      "Prediction: SPAM\n",
      "Prediction: HAM\n",
      "Prediction: HAM\n",
      "Prediction: SPAM\n",
      "Prediction: HAM\n",
      "Prediction: SPAM\n",
      "Prediction: HAM\n",
      "Prediction: SPAM\n",
      "Prediction: HAM\n",
      "Prediction: HAM\n",
      "Prediction: SPAM\n",
      "Prediction: SPAM\n",
      "Prediction: HAM\n",
      "Prediction: HAM\n",
      "Prediction: HAM\n",
      "Prediction: HAM\n",
      "Prediction: SPAM\n",
      "Prediction: HAM\n",
      "Prediction: HAM\n",
      "Prediction: HAM\n",
      "Prediction: HAM\n",
      "Prediction: HAM\n",
      "Prediction: HAM\n",
      "Prediction: HAM\n",
      "Prediction: HAM\n",
      "Prediction: HAM\n",
      "Prediction: HAM\n",
      "Prediction: HAM\n",
      "Prediction: HAM\n",
      "Prediction: HAM\n",
      "Prediction: HAM\n",
      "Prediction: SPAM\n",
      "Prediction: HAM\n",
      "Prediction: SPAM\n",
      "Prediction: HAM\n",
      "Prediction: HAM\n",
      "Prediction: HAM\n",
      "Prediction: HAM\n",
      "Prediction: HAM\n",
      "Prediction: HAM\n",
      "Prediction: SPAM\n",
      "Prediction: SPAM\n",
      "Prediction: SPAM\n",
      "Prediction: HAM\n",
      "Prediction: HAM\n",
      "Prediction: HAM\n",
      "Prediction: HAM\n",
      "Prediction: SPAM\n",
      "Prediction: HAM\n",
      "Prediction: HAM\n",
      "Prediction: HAM\n",
      "Prediction: HAM\n",
      "Prediction: SPAM\n",
      "Prediction: HAM\n",
      "Prediction: HAM\n",
      "Prediction: HAM\n",
      "Prediction: HAM\n",
      "Prediction: HAM\n",
      "Prediction: HAM\n",
      "Prediction: HAM\n",
      "Prediction: HAM\n",
      "Prediction: HAM\n",
      "Prediction: HAM\n",
      "Prediction: HAM\n",
      "Prediction: HAM\n",
      "Prediction: HAM\n",
      "Prediction: HAM\n",
      "Prediction: HAM\n",
      "Prediction: HAM\n",
      "Prediction: SPAM\n",
      "Prediction: HAM\n",
      "Prediction: HAM\n",
      "Prediction: HAM\n",
      "Prediction: HAM\n",
      "Prediction: HAM\n",
      "Prediction: HAM\n",
      "Prediction: HAM\n",
      "Prediction: HAM\n",
      "Prediction: SPAM\n",
      "Prediction: HAM\n",
      "Prediction: HAM\n",
      "Prediction: SPAM\n",
      "Prediction: HAM\n",
      "Prediction: HAM\n",
      "Prediction: HAM\n",
      "Prediction: SPAM\n",
      "Prediction: HAM\n",
      "Prediction: HAM\n",
      "Prediction: HAM\n",
      "Prediction: HAM\n",
      "Prediction: SPAM\n",
      "Prediction: SPAM\n",
      "Prediction: HAM\n",
      "Prediction: HAM\n",
      "Prediction: HAM\n",
      "Prediction: HAM\n",
      "Prediction: SPAM\n",
      "Prediction: HAM\n",
      "Prediction: HAM\n",
      "Prediction: HAM\n",
      "Prediction: HAM\n",
      "Prediction: HAM\n",
      "Prediction: HAM\n",
      "Prediction: SPAM\n",
      "Prediction: HAM\n",
      "Prediction: HAM\n",
      "Prediction: HAM\n",
      "Prediction: HAM\n",
      "Prediction: HAM\n",
      "Prediction: HAM\n",
      "Prediction: HAM\n",
      "Prediction: HAM\n",
      "Prediction: HAM\n",
      "Prediction: HAM\n",
      "Prediction: HAM\n",
      "Prediction: HAM\n",
      "Prediction: HAM\n",
      "Prediction: HAM\n",
      "Prediction: HAM\n",
      "Prediction: SPAM\n",
      "Prediction: SPAM\n",
      "Prediction: HAM\n",
      "Prediction: SPAM\n",
      "Prediction: HAM\n",
      "Prediction: HAM\n",
      "Prediction: HAM\n",
      "Prediction: HAM\n",
      "Prediction: HAM\n",
      "Prediction: HAM\n",
      "Prediction: HAM\n",
      "Prediction: SPAM\n",
      "Prediction: HAM\n",
      "Prediction: HAM\n",
      "Prediction: SPAM\n",
      "Prediction: HAM\n",
      "Prediction: HAM\n",
      "Prediction: SPAM\n",
      "Prediction: HAM\n",
      "Prediction: HAM\n",
      "Prediction: HAM\n",
      "Saved predictions to spam_ham_predictions.csv\n",
      "\n",
      "Accuracy: 86.86 %\n",
      "\n",
      "Confusion Matrix (GT rows, Pred columns)\n",
      "        HAM   SPAM\n",
      "HAM     120      5\n",
      "SPAM     18     32\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def classify_text(text: str) -> str:\n",
    "  # Minimal prompt\n",
    "  prompt = f\"\"\"\n",
    "  SMS: \"{text}\"\n",
    "  Classify as SPAM or HAM.\n",
    "  Reply with only one word: SPAM or HAM.\n",
    "  \"\"\"\n",
    "\n",
    "  inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "  with torch.no_grad():\n",
    "      outputs = model.generate(\n",
    "          **inputs,\n",
    "          max_new_tokens=5,\n",
    "          do_sample=False,\n",
    "          temperature=0.0,\n",
    "          pad_token_id=tokenizer.eos_token_id\n",
    "      )\n",
    "\n",
    "  decoded = tokenizer.decode(outputs[0][inputs[\"input_ids\"].shape[1]:], skip_special_tokens=True)\n",
    "\n",
    "  # Robust post-processing\n",
    "  decoded = decoded.upper().strip()\n",
    "\n",
    "  # Extract only SPAM or HAM from anywhere in text\n",
    "  match = re.search(r\"\\b(SPAM|HAM)\\b\", decoded)\n",
    "  if match:\n",
    "      label = match.group(1)\n",
    "  else:\n",
    "      # fallback: treat message with phone/prize/urgency as SPAM\n",
    "      if re.search(r\"\\b\\d{6,}\\b\", text) or re.search(r\"prize|win|congratulations|urgent|claim\", text, re.I):\n",
    "          label = \"SPAM\"\n",
    "      else:\n",
    "          label = \"HAM\"\n",
    "\n",
    "  print(\"Prediction:\", label)\n",
    "  return label\n",
    "\n",
    "\n",
    "# Read CSV\n",
    "# Read original CSV\n",
    "df = pd.read_csv(CSV_PATH)\n",
    "\n",
    "records = []\n",
    "\n",
    "for idx, row in df.iterrows():\n",
    "    text = row[TEXT_COLUMN]\n",
    "    gt = row[\"label\"].upper()\n",
    "\n",
    "    pred = classify_text(text)\n",
    "    records.append({\n",
    "        \"label\": gt,\n",
    "        \"text\": text,\n",
    "        \"prediction\": pred\n",
    "    })\n",
    "\n",
    "# Create new DataFrame\n",
    "pred_df = pd.DataFrame(records)\n",
    "\n",
    "# Save new CSV\n",
    "OUTPUT_CSV = \"spam_ham_predictions.csv\"\n",
    "pred_df.to_csv(OUTPUT_CSV, index=False)\n",
    "\n",
    "print(f\"Saved predictions to {OUTPUT_CSV}\")\n",
    "\n",
    "# Read prediction CSV\n",
    "df_pred = pd.read_csv(OUTPUT_CSV)\n",
    "\n",
    "# Normalize labels\n",
    "df_pred[\"label\"] = df_pred[\"label\"].str.upper()\n",
    "df_pred[\"prediction\"] = df_pred[\"prediction\"].str.upper()\n",
    "\n",
    "# Filter valid predictions only\n",
    "valid_df = df_pred[df_pred[\"prediction\"].isin([\"HAM\", \"SPAM\"])]\n",
    "\n",
    "y_true = valid_df[\"label\"]\n",
    "y_pred = valid_df[\"prediction\"]\n",
    "\n",
    "# Accuracy\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(y_true, y_pred, labels=[\"HAM\", \"SPAM\"])\n",
    "\n",
    "print(\"\\nAccuracy:\", round(accuracy * 100, 2), \"%\")\n",
    "\n",
    "print(\"\\nConfusion Matrix (GT rows, Pred columns)\")\n",
    "print(\"        HAM   SPAM\")\n",
    "print(f\"HAM   {cm[0][0]:5d}  {cm[0][1]:5d}\")\n",
    "print(f\"SPAM  {cm[1][0]:5d}  {cm[1][1]:5d}\")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyOyO6czOwgsFIlNQGFUMRTr",
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
